import { ArticleLayout } from '@/components/ArticleLayout'

export const article = {
  author: 'Cedric Clyburn',
  title: 'LLM compression and optimization: Cheaper inference with fewer hardware resources',
  date: '2024-06-30',
  description: 'Explore techniques for compressing and optimizing large language models to reduce computational costs and hardware requirements while maintaining performance.',
}

export const metadata = {
  title: article.title,
  description: article.description,
}

export default (props) => <ArticleLayout article={article} {...props} />

As large language models continue to grow in size and complexity, the computational costs and hardware requirements for inference have become significant barriers to widespread adoption. Organizations are seeking ways to deploy powerful LLMs while managing costs and resource constraints. This article explores various compression and optimization techniques that can dramatically reduce the hardware requirements for LLM inference without substantially compromising performance.

## The Challenge of LLM Resource Requirements

Modern large language models present several resource challenges:

### Memory Requirements
- **Parameter Storage**: Models like GPT-4 require hundreds of gigabytes of GPU memory
- **Activation Memory**: Intermediate computations consume additional memory
- **KV Cache**: Attention caches grow with sequence length

### Computational Costs
- **FLOPs**: Trillions of floating-point operations per inference
- **Memory Bandwidth**: Constant data movement between memory and compute units
- **Energy Consumption**: High power requirements for GPU clusters

## Compression Techniques

### Quantization

Quantization reduces the precision of model weights and activations, significantly decreasing memory usage and computational requirements.

#### Post-Training Quantization (PTQ)
Convert trained models to lower precision:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model in full precision
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

# Apply dynamic quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Model is now ~4x smaller
```

#### Quantization-Aware Training (QAT)
Train models with quantization in mind:

```python
import torch.quantization as quant

# Prepare model for QAT
model.qconfig = quant.get_default_qat_qconfig('fbgemm')
model_prepared = quant.prepare_qat(model)

# Train with quantization simulation
for epoch in range(num_epochs):
    train_one_epoch(model_prepared, train_loader)

# Convert to quantized model
quantized_model = quant.convert(model_prepared)
```

### Pruning

Remove unnecessary weights and connections to reduce model size.

#### Structured Pruning
Remove entire channels or layers:

```python
import torch.nn.utils.prune as prune

# Prune 30% of weights in each linear layer
for module in model.modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name='weight', amount=0.3)

# Make pruning permanent
for module in model.modules():
    if isinstance(module, torch.nn.Linear):
        prune.remove(module, 'weight')
```

#### Unstructured Pruning
Remove individual weights based on magnitude:

```python
# Global magnitude pruning
parameters_to_prune = []
for module in model.modules():
    if isinstance(module, torch.nn.Linear):
        parameters_to_prune.append((module, 'weight'))

prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.4,  # Remove 40% of weights globally
)
```

### Knowledge Distillation

Train smaller models to mimic larger ones:

```python
import torch.nn.functional as F

def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0, alpha=0.7):
    # Soft targets from teacher
    soft_loss = F.kl_div(
        F.log_softmax(student_logits / temperature, dim=1),
        F.softmax(teacher_logits / temperature, dim=1),
        reduction='batchmean'
    ) * (temperature ** 2)
    
    # Hard targets (original labels)
    hard_loss = F.cross_entropy(student_logits, labels)
    
    return alpha * soft_loss + (1 - alpha) * hard_loss
```

## Optimization Techniques

### Model Architecture Optimization

#### Attention Mechanisms
Optimize attention computation:

```python
# Multi-Query Attention (MQA)
class MultiQueryAttention(torch.nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.q_linear = torch.nn.Linear(d_model, d_model)
        self.k_linear = torch.nn.Linear(d_model, self.d_k)  # Single key
        self.v_linear = torch.nn.Linear(d_model, self.d_k)  # Single value
        self.out = torch.nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Multi-head queries, single key/value
        Q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.d_k)
        K = self.k_linear(key).unsqueeze(2)  # Broadcast for all heads
        V = self.v_linear(value).unsqueeze(2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention = F.softmax(scores, dim=-1)
        context = torch.matmul(attention, V)
        
        return self.out(context.view(batch_size, -1, self.num_heads * self.d_k))
```

#### Layer Sharing
Share parameters across layers:

```python
class SharedTransformerLayer(torch.nn.Module):
    def __init__(self, d_model, num_heads, num_shared_layers=3):
        super().__init__()
        self.shared_layer = TransformerLayer(d_model, num_heads)
        self.num_shared_layers = num_shared_layers
    
    def forward(self, x):
        for _ in range(self.num_shared_layers):
            x = self.shared_layer(x)
        return x
```

### Inference Optimization

#### Speculative Decoding
Use a smaller model to propose tokens, verified by the larger model:

```python
def speculative_decoding(large_model, small_model, input_ids, max_new_tokens, gamma=4):
    generated = input_ids.clone()
    
    for _ in range(max_new_tokens // gamma):
        # Small model generates gamma tokens
        draft_tokens = []
        current_input = generated
        
        for _ in range(gamma):
            with torch.no_grad():
                logits = small_model(current_input).logits[:, -1, :]
                next_token = torch.multinomial(F.softmax(logits, dim=-1), 1)
                draft_tokens.append(next_token)
                current_input = torch.cat([current_input, next_token], dim=1)
        
        # Large model verifies all tokens at once
        candidate_sequence = torch.cat(draft_tokens, dim=1)
        verification_input = torch.cat([generated, candidate_sequence], dim=1)
        
        with torch.no_grad():
            large_logits = large_model(verification_input).logits
        
        # Accept/reject tokens based on probability ratios
        accepted_tokens = verify_tokens(large_logits, candidate_sequence, small_model)
        generated = torch.cat([generated, accepted_tokens], dim=1)
    
    return generated
```

#### Dynamic Batching
Optimize batch processing for variable-length sequences:

```python
class DynamicBatcher:
    def __init__(self, max_batch_size=32, max_wait_time=0.1):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
    
    def add_request(self, request):
        self.pending_requests.append(request)
        
        if len(self.pending_requests) >= self.max_batch_size:
            return self.process_batch()
        return None
    
    def process_batch(self):
        if not self.pending_requests:
            return None
        
        batch = self.pending_requests[:self.max_batch_size]
        self.pending_requests = self.pending_requests[self.max_batch_size:]
        
        # Pad sequences to same length
        max_len = max(len(req.input_ids) for req in batch)
        padded_inputs = []
        
        for req in batch:
            padded = F.pad(req.input_ids, (0, max_len - len(req.input_ids)))
            padded_inputs.append(padded)
        
        return torch.stack(padded_inputs)
```

## Hardware-Specific Optimizations

### GPU Optimization

#### CUDA Kernels
Optimize specific operations:

```python
# Custom CUDA kernel for fused operations
import triton
import triton.language as tl

@triton.jit
def fused_attention_kernel(
    Q, K, V, Out,
    stride_qm, stride_qk,
    stride_km, stride_kk,
    stride_vm, stride_vk,
    stride_om, stride_ok,
    M, K,
    BLOCK_M: tl.constexpr, BLOCK_K: tl.constexpr,
):
    # Fused attention computation
    pid = tl.program_id(0)
    
    # Load Q, K, V blocks
    q_offset = pid * BLOCK_M * stride_qm + tl.arange(0, BLOCK_M)[:, None] * stride_qm + tl.arange(0, BLOCK_K)[None, :] * stride_qk
    k_offset = tl.arange(0, BLOCK_K)[:, None] * stride_km + tl.arange(0, BLOCK_K)[None, :] * stride_kk
    v_offset = tl.arange(0, BLOCK_K)[:, None] * stride_vm + tl.arange(0, BLOCK_K)[None, :] * stride_vk
    
    q = tl.load(Q + q_offset)
    k = tl.load(K + k_offset)
    v = tl.load(V + v_offset)
    
    # Compute attention
    scores = tl.dot(q, k)
    scores = tl.softmax(scores, axis=1)
    out = tl.dot(scores, v)
    
    # Store result
    out_offset = pid * BLOCK_M * stride_om + tl.arange(0, BLOCK_M)[:, None] * stride_om + tl.arange(0, BLOCK_K)[None, :] * stride_ok
    tl.store(Out + out_offset, out)
```

#### Memory Management
Optimize GPU memory usage:

```python
class GPUMemoryManager:
    def __init__(self):
        self.memory_pool = {}
        self.allocation_tracker = {}
    
    def allocate(self, size, dtype):
        key = (size, dtype)
        
        if key in self.memory_pool and self.memory_pool[key]:
            tensor = self.memory_pool[key].pop()
            tensor.zero_()
            return tensor
        
        tensor = torch.empty(size, dtype=dtype, device='cuda')
        self.allocation_tracker[id(tensor)] = key
        return tensor
    
    def deallocate(self, tensor):
        tensor_id = id(tensor)
        if tensor_id in self.allocation_tracker:
            key = self.allocation_tracker[tensor_id]
            if key not in self.memory_pool:
                self.memory_pool[key] = []
            self.memory_pool[key].append(tensor)
            del self.allocation_tracker[tensor_id]
```

### CPU Optimization

#### SIMD Instructions
Leverage vectorized operations:

```cpp
// Example C++ code for vectorized operations
#include <immintrin.h>

void vectorized_multiply(float* a, float* b, float* c, int n) {
    for (int i = 0; i < n; i += 8) {
        __m256 va = _mm256_load_ps(&a[i]);
        __m256 vb = _mm256_load_ps(&b[i]);
        __m256 vc = _mm256_mul_ps(va, vb);
        _mm256_store_ps(&c[i], vc);
    }
}
```

## Practical Implementation Strategies

### Gradual Optimization Approach

1. **Baseline Measurement**: Establish performance and accuracy baselines
2. **Quantization**: Apply post-training quantization first
3. **Pruning**: Remove redundant parameters
4. **Distillation**: Train smaller models if needed
5. **Hardware Optimization**: Apply platform-specific optimizations

### Monitoring and Validation

```python
class OptimizationValidator:
    def __init__(self, original_model, test_dataset):
        self.original_model = original_model
        self.test_dataset = test_dataset
        self.baseline_metrics = self.evaluate_model(original_model)
    
    def evaluate_model(self, model):
        total_loss = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in self.test_dataset:
                outputs = model(**batch)
                total_loss += outputs.loss.item() * batch['input_ids'].size(0)
                total_samples += batch['input_ids'].size(0)
        
        return {
            'perplexity': torch.exp(torch.tensor(total_loss / total_samples)),
            'model_size': sum(p.numel() for p in model.parameters()),
            'memory_usage': torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0
        }
    
    def validate_optimization(self, optimized_model, max_perplexity_increase=0.05):
        optimized_metrics = self.evaluate_model(optimized_model)
        
        perplexity_increase = (optimized_metrics['perplexity'] - self.baseline_metrics['perplexity']) / self.baseline_metrics['perplexity']
        size_reduction = 1 - (optimized_metrics['model_size'] / self.baseline_metrics['model_size'])
        
        return {
            'perplexity_increase': perplexity_increase,
            'size_reduction': size_reduction,
            'acceptable': perplexity_increase <= max_perplexity_increase
        }
```

## Cost-Benefit Analysis

### Resource Savings

Typical optimization results:
- **Quantization (INT8)**: 4x memory reduction, 2-3x speedup
- **Pruning (50%)**: 2x model size reduction, 1.5x speedup
- **Distillation**: 10-100x parameter reduction, 5-20x speedup
- **Combined optimizations**: Up to 100x resource reduction

### Performance Trade-offs

Consider the trade-offs:
- **Accuracy**: Small degradation (1-5%) is often acceptable
- **Latency**: Optimization usually improves inference speed
- **Throughput**: Better resource utilization increases throughput
- **Energy**: Significant power consumption reduction

## Conclusion

LLM compression and optimization techniques offer powerful ways to make large language models more accessible and cost-effective. By combining quantization, pruning, knowledge distillation, and hardware-specific optimizations, organizations can achieve significant resource savings while maintaining acceptable performance levels.

The key to successful optimization lies in understanding the specific requirements of your use case and applying techniques incrementally while carefully monitoring performance. As these optimization techniques continue to mature, we can expect even more dramatic improvements in the efficiency of LLM deployment.

The future of AI deployment will increasingly depend on our ability to optimize these models effectively, making powerful AI capabilities available to a broader range of organizations and applications.

---

*Originally published on [Red Hat Blog](https://www.redhat.com/en/blog/llm-compression-and-optimization-cheaper-inference-with-fewer-hardware-resources)*
